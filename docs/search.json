[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vikram Vasan CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\nFeb 17, 2025\n\n\nVikram Vasan\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The first task for this project is to explore the data. This will be done by creating a table of summary statistics for the data set. In order to do this we want to not only look at the quantitative data, but also the relevant qualitative data. In order to do this, for the binary data (sex and clutch completion) we can simply convert the type to an integer. For the island, it is a bit more complicated since there are 3 islands, so we will need to count the number of each island. We then simply group by the species and are able to get the averages for each variable for each species.\n\ndf_stats = train.copy()\ndf_stats[\"Female\"] = (df_stats[\"Sex\"] == \"FEMALE\").astype(int)\ndf_stats[\"Clutch Completion\"] = (df_stats[\"Clutch Completion\"] == \"Yes\").astype(int)\nsummary_table = df_stats.groupby(\"Species\").mean(numeric_only=True).drop(columns=[\"Sample Number\"])\nisland_percentages = df_stats.groupby(\"Species\")[\"Island\"].value_counts(normalize=True).unstack(fill_value=0)\nfinal_summary = summary_table.join(island_percentages)\nfinal_summary.reset_index(inplace=True)\n\ndisplay(final_summary)\n\n\n\n\n\n\n\n\nSpecies\nClutch Completion\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nFemale\nBiscoe\nDream\nTorgersen\n\n\n\n\n0\nAdelie\n0.900000\n38.970588\n18.409244\n190.084034\n3718.487395\n8.861824\n-25.796897\n0.441667\n0.275\n0.375\n0.35\n\n\n1\nChinstrap\n0.824561\n48.826316\n18.366667\n196.000000\n3743.421053\n9.331004\n-24.553401\n0.543860\n0.000\n1.000\n0.00\n\n\n2\nGentoo\n0.918367\n47.073196\n14.914433\n216.752577\n5039.948454\n8.247341\n-26.149389\n0.500000\n1.000\n0.000\n0.00\n\n\n\n\n\n\n\nLooking at the summary statistics we immediately get a clue as to which variables may or may not be useful in predicting the species. For example, Chinstrap and Gentoo penguins, only exist on one island each which, indicating that training based on the island will be very valuable. Additionally, Gentoo penguins are significantly larger with longer flippers on average while Adelie’s have much shorter Culmens than the others. The sex measurements, on the other hand, do not appear to be very useful as their distributions are more similar across the species.\n\n\n\nTo get a better sense of some of the quantitative variables, we can visualize pairs of them in scatter plots. This will go further than simply using the means to see if there are any clear patterns between species in the data. We will look at the Culmen sizes, flipper lengths, body masses, and blood measurements.\n\nfig, ax = plt.subplots(1, 3, figsize = (17, 3.5))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", style = \"Species\", ax = ax[0])\np1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\", style = \"Species\", ax = ax[1])\np2 = sns.scatterplot(train, x = \"Delta 15 N (o/oo)\", y = \"Delta 13 C (o/oo)\",  hue = \"Species\", style = \"Species\", ax = ax[2])\n\n\n\n\n\n\n\n\nLooking at these plots, we can see that the species are not perfectly separable on any one variable however there are clear trends. For example, the Culmen sizes form 3 pretty clear clusters, indicating that these variables, when used together, will be useful in predicting the species. The body mass and flipper length plot is also interesting as Gentoo penguins are clearly separated with Chinstrap and Adelie coming in at very similar sizes. The blood measurements are not as useful as the other variables as there is a lot of overlap between the species, but some vague groups still appear."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#summary-statistics",
    "href": "posts/classifying-palmer-penguins/index.html#summary-statistics",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The first task for this project is to explore the data. This will be done by creating a table of summary statistics for the data set. In order to do this we want to not only look at the quantitative data, but also the relevant qualitative data. In order to do this, for the binary data (sex and clutch completion) we can simply convert the type to an integer. For the island, it is a bit more complicated since there are 3 islands, so we will need to count the number of each island. We then simply group by the species and are able to get the averages for each variable for each species.\n\ndf_stats = train.copy()\ndf_stats[\"Female\"] = (df_stats[\"Sex\"] == \"FEMALE\").astype(int)\ndf_stats[\"Clutch Completion\"] = (df_stats[\"Clutch Completion\"] == \"Yes\").astype(int)\nsummary_table = df_stats.groupby(\"Species\").mean(numeric_only=True).drop(columns=[\"Sample Number\"])\nisland_percentages = df_stats.groupby(\"Species\")[\"Island\"].value_counts(normalize=True).unstack(fill_value=0)\nfinal_summary = summary_table.join(island_percentages)\nfinal_summary.reset_index(inplace=True)\n\ndisplay(final_summary)\n\n\n\n\n\n\n\n\nSpecies\nClutch Completion\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nFemale\nBiscoe\nDream\nTorgersen\n\n\n\n\n0\nAdelie\n0.900000\n38.970588\n18.409244\n190.084034\n3718.487395\n8.861824\n-25.796897\n0.441667\n0.275\n0.375\n0.35\n\n\n1\nChinstrap\n0.824561\n48.826316\n18.366667\n196.000000\n3743.421053\n9.331004\n-24.553401\n0.543860\n0.000\n1.000\n0.00\n\n\n2\nGentoo\n0.918367\n47.073196\n14.914433\n216.752577\n5039.948454\n8.247341\n-26.149389\n0.500000\n1.000\n0.000\n0.00\n\n\n\n\n\n\n\nLooking at the summary statistics we immediately get a clue as to which variables may or may not be useful in predicting the species. For example, Chinstrap and Gentoo penguins, only exist on one island each which, indicating that training based on the island will be very valuable. Additionally, Gentoo penguins are significantly larger with longer flippers on average while Adelie’s have much shorter Culmens than the others. The sex measurements, on the other hand, do not appear to be very useful as their distributions are more similar across the species."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#visualizations",
    "href": "posts/classifying-palmer-penguins/index.html#visualizations",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "To get a better sense of some of the quantitative variables, we can visualize pairs of them in scatter plots. This will go further than simply using the means to see if there are any clear patterns between species in the data. We will look at the Culmen sizes, flipper lengths, body masses, and blood measurements.\n\nfig, ax = plt.subplots(1, 3, figsize = (17, 3.5))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", style = \"Species\", ax = ax[0])\np1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\", style = \"Species\", ax = ax[1])\np2 = sns.scatterplot(train, x = \"Delta 15 N (o/oo)\", y = \"Delta 13 C (o/oo)\",  hue = \"Species\", style = \"Species\", ax = ax[2])\n\n\n\n\n\n\n\n\nLooking at these plots, we can see that the species are not perfectly separable on any one variable however there are clear trends. For example, the Culmen sizes form 3 pretty clear clusters, indicating that these variables, when used together, will be useful in predicting the species. The body mass and flipper length plot is also interesting as Gentoo penguins are clearly separated with Chinstrap and Adelie coming in at very similar sizes. The blood measurements are not as useful as the other variables as there is a lot of overlap between the species, but some vague groups still appear."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#data-preprocessing",
    "href": "posts/classifying-palmer-penguins/index.html#data-preprocessing",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nNow that we have more of an idea about each variable, the next step is to train models to determine the most predictive variables. To do this, we first need to preprocess the data to binarize the categorical variables so that they can be used for the models. We will use one-hot encoding for these variables, as shown below.\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  x = df.drop([\"Species\"], axis = 1)\n  x = pd.get_dummies(df)\n  return x, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nSpecies_Adelie\nSpecies_Chinstrap\nSpecies_Gentoo\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#variable-and-model-selection",
    "href": "posts/classifying-palmer-penguins/index.html#variable-and-model-selection",
    "title": "Classifying Palmer Penguins",
    "section": "Variable and Model Selection",
    "text": "Variable and Model Selection\nTo determine the best variable combinations, we will run some tests on different combinations of 3 variables. To do this we first generate every combination of 2 numeric and 1 categorical variable, which the following lines do so that the combinations can be used later.\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncombs = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    combs.append(cols)\n\nWe then loop through the combinations and use cross-validation scoring to determine which combinations perform the best. We not only want to test the variable combinations, we also want to test different models on each combination to see which one is the best. We will test the following models: Logistic Regression, Decision Tree, Random Forest, and Support Vector Machines. We will then use the best model and variables to predict the species of the test data. We test the variable combinations and models simultaneously by performing the cross-validation tests on each model for each combination.\n\nwarnings.filterwarnings('ignore')\n\ntests = []\n\nfor cols in combs:\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    LR.score(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    DTC = DecisionTreeClassifier()\n    DTC.fit(X_train[cols], y_train)\n    DTC.score(X_train[cols], y_train)\n    cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n    RFC = RandomForestClassifier()\n    RFC.fit(X_train[cols], y_train)\n    RFC.score(X_train[cols], y_train)\n    cv_scores_RFC = cross_val_score(RFC, X_train[cols], y_train, cv = 5)\n    SVM = SVC()\n    SVM.fit(X_train[cols], y_train)\n    SVM.score(X_train[cols], y_train)\n    cv_scores_SVM = cross_val_score(SVM, X_train[cols], y_train, cv = 5)\n    tests.append((cols, cv_scores_LR.mean(), cv_scores_DTC.mean(), cv_scores_RFC.mean(), cv_scores_SVM.mean()))\n\nTo display the outcome of these tests we use Pandas to output the average cross-validation score for each model and variable combination. Outputting just the top 5 since the number of combinations is unwieldy, we can observe some interesting things about the combinations and models.\n\ndf_results = pd.DataFrame(tests, columns=[\"Columns\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"])\n\ndf_results[\"Columns\"] = df_results[\"Columns\"].apply(lambda x: \", \".join(x))\n\npd.set_option(\"display.max_colwidth\", None)  # no truncation of column text\n\ndf_results = df_results.sort_values(by=\"Logistic Regression\", ascending=False) # sort by cross-val score\ndf_results.reset_index(drop=True, inplace=True)\n\ndf_results.head()\n\n\n\n\n\n\n\n\nColumns\nLogistic Regression\nDecision Tree\nRandom Forest\nSVM\n\n\n\n\n0\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n0.996154\n0.976546\n0.980468\n0.820362\n\n\n1\nCulmen Length (mm), Culmen Depth (mm), Sex_FEMALE, Sex_MALE\n0.984465\n0.972624\n0.980468\n0.808673\n\n\n2\nCulmen Length (mm), Delta 13 C (o/oo), Island_Biscoe, Island_Dream, Island_Torgersen\n0.972624\n0.964932\n0.972700\n0.722700\n\n\n3\nCulmen Length (mm), Delta 15 N (o/oo), Island_Biscoe, Island_Dream, Island_Torgersen\n0.964857\n0.968778\n0.972700\n0.730543\n\n\n4\nCulmen Length (mm), Culmen Depth (mm), Clutch Completion_No, Clutch Completion_Yes\n0.957014\n0.945249\n0.949170\n0.820362\n\n\n\n\n\n\n\nFor example, the Logistic Regression tends to perform the best with SVM much inferior and the Random Forests slightly better that Decision Trees (which makes sense since Random Forests are just an extension of Decision Trees). The best variable combinations are also interesting as they are not always the same for each model. For example, the best combination for Logistic Regression is Culmen Length, Culmen Depth, and the Islands, while for Random Forests it is Culmen Length, Flipper Length, and Sex. This indicates that the models are learning different things from the data and that the best combination is not always the same. That said, as we predicted following the initial data exploration, the Culmen sizes and island are very important in predicting the species. The overall best combination was the Logistic Regression with Culmen Length, Culmen Depth, and the Islands, which had an average cross-validation score of 0.996."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#results",
    "href": "posts/classifying-palmer-penguins/index.html#results",
    "title": "Classifying Palmer Penguins",
    "section": "Results",
    "text": "Results\nWe then want to use these variables to train each model with the training data. We use the trained model to predict the species from the test data and compare the results to the actual species. We also test the models on the training data to see if they are overfitting. We can then output the accuracy of the model on the test and training data.\n\nX_test, y_test = prepare_data(test)\nX_test.head()\nLR = LogisticRegression()\nLR.fit(X_train[LR_cols], y_train)\nLR_train_score = LR.score(X_train[LR_cols], y_train)\nLR_test_score = LR.score(X_test[LR_cols], y_test)\n\nDTC = DecisionTreeClassifier()\nDTC.fit(X_train[DT_cols], y_train)\nDT_train_score = DTC.score(X_train[DT_cols], y_train)\nDT_test_score = DTC.score(X_test[DT_cols], y_test)\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[RF_cols], y_train)\nRF_train_score = RFC.score(X_train[RF_cols], y_train)\nRF_test_score = RFC.score(X_test[RF_cols], y_test)\n\nSVM = SVC()\nSVM.fit(X_train[SVM_cols], y_train)\nSVM_train_score = SVM.score(X_train[SVM_cols], y_train)\nSVM_test_score = SVM.score(X_test[SVM_cols], y_test)\n\nscores = [[LR_train_score, LR_test_score, LR_cols], [DT_train_score, DT_test_score, DT_cols], [RF_train_score, RF_test_score, RF_cols], [SVM_train_score, SVM_test_score, SVM_cols]]\n\ndf_scores = pd.DataFrame(scores, columns=[\"Training Accuracy\", \"Testing Accuracy\", \"Variables\"])\ndf_scores[\"Model\"] = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"]\ndf_scores[\"Training Accuracy\"] = df_scores[\"Training Accuracy\"].round(3)\ndf_scores[\"Testing Accuracy\"] = df_scores[\"Testing Accuracy\"].round(3)\ndf_scores[\"Variables\"] = df_scores[\"Variables\"].apply(lambda x: \", \".join(x))\ndf_scores = df_scores[[\"Model\", \"Training Accuracy\", \"Testing Accuracy\", \"Variables\"]]\ndisplay(df_scores)\n\n\n\n\n\n\n\n\nModel\nTraining Accuracy\nTesting Accuracy\nVariables\n\n\n\n\n0\nLogistic Regression\n0.996\n1.000\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n1\nDecision Tree\n1.000\n0.985\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n2\nRandom Forest\n1.000\n0.971\nCulmen Length (mm), Flipper Length (mm), Sex_FEMALE, Sex_MALE\n\n\n3\nSVM\n0.891\n0.941\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n\n\n\n\n\nHere, we see that the Logistic Regression model with Culmen Length, Culmen Depth, and Island achieves an impressive 100% testing accuracy which is exactly what we were looking for. The training accuracy is also very high at 99.6% which indicates that the model is not overfitting. The other models also perform well, but not as well as the Logistic Regression model. The Decision Tree model with same variables is the next best with a testing accuracy of 98.5% and a training accuracy of 100%. The Random Forest model with Culmen Length, Flipper Length, and Sex also did well with a testing accuracy of 97.1% and a training accuracy of 100%. The SVM model did the worse with a testing accuracy of 94.1% and a training accuracy of 89.1%.\nIt is interesting that the best variables for 3 of the 4 models is Culmen Length, Culmen Depth, and Island. This indicates that these variables are the most important in predicting the species. The Random Forest model is the only one that has a different best variable combination, which is Culmen Length, Flipper Length, and Sex, which is interesting as it indicates that the model is learning something different from the data. The Logistic Regression model is the best overall, which is not surprising as it was the best in the cross-validation tests."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#visualizations-1",
    "href": "posts/classifying-palmer-penguins/index.html#visualizations-1",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations",
    "text": "Visualizations\nNow that we know that the Logistic Regression model with Culmen Length, Culmen Depth, and Island as variables is the best model, we can visualize the results. We plot the Culmen length and depth split by island and colored by species for both the training and test data. We also show the decision regions as determined by the model.\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i].replace(\"_\", \" \"))\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[LR_cols], y_train)\nplot_regions(LR, X_test[LR_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows that the model is able to separate the species very well. The decision boundaries are very clear and the model is able to predict the species in the testing data with 100% accuracy. This is a very good result and indicates that the model is quite good at predicting the species of penguins using only the chosen variables. It is also clear that, as we expected, the Island is very helpful as Gentoo penguins are the only species that exist across all 3 islands, greatly simplifying the prediction complexity.\n\ny_test_pred = LR.predict(X_test[LR_cols])\nC = confusion_matrix(y_test, y_test_pred)\n\nclass_labels = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n\nplt.figure(figsize=(6,4))\nsns.heatmap(C, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we want to take a look at the confusion matrix. Since we had 100% test accuracy, we expect the confusion matrix to be a diagonal matrix with numbers on the diagonal and 0s elsewhere. This is exactly what we see, which is a good sign that the model is working well. All of the species are predicted perfectly, which is exactly what we were looking for."
  }
]