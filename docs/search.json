[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vikram Vasan CSCI 0451 Blog",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\n\n\n\n\n\nBlog Post 4\n\n\n\n\n\nMar 24, 2025\n\n\nVikram Vasan\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nBlog Post 3\n\n\n\n\n\nMar 4, 2025\n\n\nVikram Vasan\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\nFeb 24, 2025\n\n\nVikram Vasan\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\nFeb 20, 2025\n\n\nVikram Vasan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this blog post, we explore the classification of Palmer Penguins using various machine learning models. We will discuss the dataset, preprocessing steps, model training, and evaluation. The goal of this project is to predict the species of penguins based on a subset of the available features in the Palmer Penguins dataset. There are several variables available including Culmen size, flipper length, body mass, and the island the penguin was found on, among others. We will test different combinations of quantitative and qualitative features to determine which features are most important for classification. We will also compare the performance of different machine learning models to determine which model is best suited for this task. We will test Logistic Regression, Decision Trees, Random Forest, and Support Vector Machines, using cross-validation to ensure that our results are robust. Once the best features and model is determined, we will test the model on the test set to evaluate its performance and discuss the results using plots and performance metrics."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#summary-statistics",
    "href": "posts/classifying-palmer-penguins/index.html#summary-statistics",
    "title": "Classifying Palmer Penguins",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nThe first task for this project is to explore the data. This will be done by creating a table of summary statistics for the data set. In order to do this we want to not only look at the quantitative data, but also the relevant qualitative data. In order to do this, for the binary data (sex and clutch completion) we can simply convert the type to an integer. For the island, it is a bit more complicated since there are 3 islands, so we will need to count the number of each island. We then simply group by the species and are able to get the averages for each variable for each species.\n\ndf_stats = train.copy()\ndf_stats[\"Female\"] = (df_stats[\"Sex\"] == \"FEMALE\").astype(int)\ndf_stats[\"Clutch Completion\"] = (df_stats[\"Clutch Completion\"] == \"Yes\").astype(int)\nsummary_table = df_stats.groupby(\"Species\").mean(numeric_only=True).drop(columns=[\"Sample Number\"])\nisland_percentages = df_stats.groupby(\"Species\")[\"Island\"].value_counts(normalize=True).unstack(fill_value=0)\nfinal_summary = summary_table.join(island_percentages)\nfinal_summary.reset_index(inplace=True)\n\ndisplay(final_summary)\n\n\n\n\n\n\n\n\nSpecies\nClutch Completion\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nFemale\nBiscoe\nDream\nTorgersen\n\n\n\n\n0\nAdelie\n0.900000\n38.970588\n18.409244\n190.084034\n3718.487395\n8.861824\n-25.796897\n0.441667\n0.275\n0.375\n0.35\n\n\n1\nChinstrap\n0.824561\n48.826316\n18.366667\n196.000000\n3743.421053\n9.331004\n-24.553401\n0.543860\n0.000\n1.000\n0.00\n\n\n2\nGentoo\n0.918367\n47.073196\n14.914433\n216.752577\n5039.948454\n8.247341\n-26.149389\n0.500000\n1.000\n0.000\n0.00\n\n\n\n\n\n\n\nLooking at the summary statistics we immediately get a clue as to which variables may or may not be useful in predicting the species. For example, Chinstrap and Gentoo penguins, only exist on one island each which, indicating that training based on the island will be very valuable. Additionally, Gentoo penguins are significantly larger with longer flippers on average while Adelie’s have much shorter Culmens than the others. The sex measurements, on the other hand, do not appear to be very useful as their distributions are more similar across the species."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#visualizations",
    "href": "posts/classifying-palmer-penguins/index.html#visualizations",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations",
    "text": "Visualizations\nTo get a better sense of some of the quantitative variables, we can visualize pairs of them in scatter plots. This will go further than simply using the means to see if there are any clear patterns between species in the data. We will look at the Culmen sizes, flipper lengths, body masses, and blood measurements.\n\nfig, ax = plt.subplots(1, 3, figsize = (17, 3.5))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", style = \"Species\", ax = ax[0])\np1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\", style = \"Species\", ax = ax[1])\np2 = sns.scatterplot(train, x = \"Delta 15 N (o/oo)\", y = \"Delta 13 C (o/oo)\",  hue = \"Species\", style = \"Species\", ax = ax[2])\n\n\n\n\n\n\n\n\nLooking at these plots, we can see that the species are not perfectly separable on any one variable however there are clear trends. For example, the Culmen sizes form 3 pretty clear clusters, indicating that these variables, when used together, will be useful in predicting the species. The body mass and flipper length plot is also interesting as Gentoo penguins are clearly separated with Chinstrap and Adelie coming in at very similar sizes. The blood measurements are not as useful as the other variables as there is a lot of overlap between the species, but some vague groups still appear."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#data-preprocessing",
    "href": "posts/classifying-palmer-penguins/index.html#data-preprocessing",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nNow that we have more of an idea about each variable, the next step is to train models to determine the most predictive variables. To do this, we first need to preprocess the data to binarize the categorical variables so that they can be used for the models. We will use one-hot encoding for these variables, as shown below.\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  x = df.drop([\"Species\"], axis = 1)\n  x = pd.get_dummies(df)\n  return x, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nSpecies_Adelie\nSpecies_Chinstrap\nSpecies_Gentoo\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#variable-and-model-selection",
    "href": "posts/classifying-palmer-penguins/index.html#variable-and-model-selection",
    "title": "Classifying Palmer Penguins",
    "section": "Variable and Model Selection",
    "text": "Variable and Model Selection\nTo determine the best variable combinations, we will run some tests on different combinations of 3 variables. To do this we first generate every combination of 2 numeric and 1 categorical variable, which the following lines do so that the combinations can be used later.\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncombs = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    combs.append(cols)\n\nWe then loop through the combinations and use cross-validation scoring to determine which combinations perform the best. We not only want to test the variable combinations, we also want to test different models on each combination to see which one is the best. We will test the following models: Logistic Regression, Decision Tree, Random Forest, and Support Vector Machines. We will then use the best model and variables to predict the species of the test data. We test the variable combinations and models simultaneously by performing the cross-validation tests on each model for each combination.\n\nwarnings.filterwarnings('ignore')\n\ntests = []\n\nfor cols in combs:\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    LR.score(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    DTC = DecisionTreeClassifier()\n    DTC.fit(X_train[cols], y_train)\n    DTC.score(X_train[cols], y_train)\n    cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n    RFC = RandomForestClassifier()\n    RFC.fit(X_train[cols], y_train)\n    RFC.score(X_train[cols], y_train)\n    cv_scores_RFC = cross_val_score(RFC, X_train[cols], y_train, cv = 5)\n    SVM = SVC()\n    SVM.fit(X_train[cols], y_train)\n    SVM.score(X_train[cols], y_train)\n    cv_scores_SVM = cross_val_score(SVM, X_train[cols], y_train, cv = 5)\n    tests.append((cols, cv_scores_LR.mean(), cv_scores_DTC.mean(), cv_scores_RFC.mean(), cv_scores_SVM.mean()))\n\nTo display the outcome of these tests we use Pandas to output the average cross-validation score for each model and variable combination. Outputting just the top 5 since the number of combinations is unwieldy, we can observe some interesting things about the combinations and models.\n\ndf_results = pd.DataFrame(tests, columns=[\"Columns\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"])\n\ndf_results[\"Columns\"] = df_results[\"Columns\"].apply(lambda x: \", \".join(x))\n\npd.set_option(\"display.max_colwidth\", None)  # no truncation of column text\n\ndf_results = df_results.sort_values(by=\"Logistic Regression\", ascending=False) # sort by cross-val score\ndf_results.reset_index(drop=True, inplace=True)\n\ndf_results.head()\n\n\n\n\n\n\n\n\nColumns\nLogistic Regression\nDecision Tree\nRandom Forest\nSVM\n\n\n\n\n0\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n0.996154\n0.976546\n0.980468\n0.820362\n\n\n1\nCulmen Length (mm), Culmen Depth (mm), Sex_FEMALE, Sex_MALE\n0.984465\n0.972624\n0.980468\n0.808673\n\n\n2\nCulmen Length (mm), Delta 13 C (o/oo), Island_Biscoe, Island_Dream, Island_Torgersen\n0.972624\n0.964932\n0.972700\n0.722700\n\n\n3\nCulmen Length (mm), Delta 15 N (o/oo), Island_Biscoe, Island_Dream, Island_Torgersen\n0.964857\n0.968778\n0.972700\n0.730543\n\n\n4\nCulmen Length (mm), Culmen Depth (mm), Clutch Completion_No, Clutch Completion_Yes\n0.957014\n0.945249\n0.949170\n0.820362\n\n\n\n\n\n\n\nFor example, the Logistic Regression tends to perform the best with SVM much inferior and the Random Forests slightly better that Decision Trees (which makes sense since Random Forests are just an extension of Decision Trees). The best variable combinations are also interesting as they are not always the same for each model. For example, the best combination for Logistic Regression is Culmen Length, Culmen Depth, and the Islands, while for Random Forests it is Culmen Length, Flipper Length, and Sex. This indicates that the models are learning different things from the data and that the best combination is not always the same. That said, as we predicted following the initial data exploration, the Culmen sizes and island are very important in predicting the species. The overall best combination was the Logistic Regression with Culmen Length, Culmen Depth, and the Islands, which had an average cross-validation score of 0.996."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#results",
    "href": "posts/classifying-palmer-penguins/index.html#results",
    "title": "Classifying Palmer Penguins",
    "section": "Results",
    "text": "Results\nWe then want to use these variables to train each model with the training data. We use the trained model to predict the species from the test data and compare the results to the actual species. We also test the models on the training data to see if they are overfitting. We can then output the accuracy of the model on the test and training data.\n\nX_test, y_test = prepare_data(test)\nX_test.head()\nLR = LogisticRegression()\nLR.fit(X_train[LR_cols], y_train)\nLR_train_score = LR.score(X_train[LR_cols], y_train)\nLR_test_score = LR.score(X_test[LR_cols], y_test)\n\nDTC = DecisionTreeClassifier()\nDTC.fit(X_train[DT_cols], y_train)\nDT_train_score = DTC.score(X_train[DT_cols], y_train)\nDT_test_score = DTC.score(X_test[DT_cols], y_test)\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[RF_cols], y_train)\nRF_train_score = RFC.score(X_train[RF_cols], y_train)\nRF_test_score = RFC.score(X_test[RF_cols], y_test)\n\nSVM = SVC()\nSVM.fit(X_train[SVM_cols], y_train)\nSVM_train_score = SVM.score(X_train[SVM_cols], y_train)\nSVM_test_score = SVM.score(X_test[SVM_cols], y_test)\n\nscores = [[LR_train_score, LR_test_score, LR_cols], [DT_train_score, DT_test_score, DT_cols], [RF_train_score, RF_test_score, RF_cols], [SVM_train_score, SVM_test_score, SVM_cols]]\n\ndf_scores = pd.DataFrame(scores, columns=[\"Training Accuracy\", \"Testing Accuracy\", \"Variables\"])\ndf_scores[\"Model\"] = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"]\ndf_scores[\"Training Accuracy\"] = df_scores[\"Training Accuracy\"].round(3)\ndf_scores[\"Testing Accuracy\"] = df_scores[\"Testing Accuracy\"].round(3)\ndf_scores[\"Variables\"] = df_scores[\"Variables\"].apply(lambda x: \", \".join(x))\ndf_scores = df_scores[[\"Model\", \"Training Accuracy\", \"Testing Accuracy\", \"Variables\"]]\ndisplay(df_scores)\n\n\n\n\n\n\n\n\nModel\nTraining Accuracy\nTesting Accuracy\nVariables\n\n\n\n\n0\nLogistic Regression\n0.996\n1.000\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n1\nDecision Tree\n1.000\n0.985\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n2\nRandom Forest\n1.000\n0.971\nCulmen Length (mm), Flipper Length (mm), Sex_FEMALE, Sex_MALE\n\n\n3\nSVM\n0.891\n0.941\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n\n\n\n\n\nHere, we see that the Logistic Regression model with Culmen Length, Culmen Depth, and Island achieves an impressive 100% testing accuracy which is exactly what we were looking for. The training accuracy is also very high at 99.6% which indicates that the model is not overfitting. The other models also perform well, but not as well as the Logistic Regression model. The Decision Tree model with same variables is the next best with a testing accuracy of 98.5% and a training accuracy of 100%. The Random Forest model with Culmen Length, Flipper Length, and Sex also did well with a testing accuracy of 97.1% and a training accuracy of 100%. The SVM model did the worse with a testing accuracy of 94.1% and a training accuracy of 89.1%.\nIt is interesting that the best variables for 3 of the 4 models is Culmen Length, Culmen Depth, and Island. This indicates that these variables are the most important in predicting the species. The Random Forest model is the only one that has a different best variable combination, which is Culmen Length, Flipper Length, and Sex, which is interesting as it indicates that the model is learning something different from the data. The Logistic Regression model is the best overall, which is not surprising as it was the best in the cross-validation tests."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#visualizations-1",
    "href": "posts/classifying-palmer-penguins/index.html#visualizations-1",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations",
    "text": "Visualizations\nNow that we know that the Logistic Regression model with Culmen Length, Culmen Depth, and Island as variables is the best model, we can visualize the results. We plot the Culmen length and depth split by island and colored by species for both the training and test data. We also show the decision regions as determined by the model.\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i].replace(\"_\", \" \"))\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[LR_cols], y_train)\nplot_regions(LR, X_test[LR_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows that the model is able to separate the species very well. The decision boundaries are very clear and the model is able to predict the species in the testing data with 100% accuracy. This is a very good result and indicates that the model is quite good at predicting the species of penguins using only the chosen variables. It is also clear that, as we expected, the Island is very helpful as Gentoo penguins are the only species that exist across all 3 islands, greatly simplifying the prediction complexity.\n\ny_test_pred = LR.predict(X_test[LR_cols])\nC = confusion_matrix(y_test, y_test_pred)\n\nclass_labels = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n\nplt.figure(figsize=(6,4))\nsns.heatmap(C, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we want to take a look at the confusion matrix. Since we had 100% test accuracy, we expect the confusion matrix to be a diagonal matrix with numbers on the diagonal and 0s elsewhere. This is exactly what we see, which is a good sign that the model is working well. All of the species are predicted perfectly, which is exactly what we were looking for."
  },
  {
    "objectID": "posts/design-and-impact of-automated-decision-systems/index.html",
    "href": "posts/design-and-impact of-automated-decision-systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "In this blog post, we explore the classification of Palmer Penguins using various machine learning models. We will discuss the dataset, preprocessing steps, model training, and evaluation. The goal of this project is to predict the species of penguins based on a subset of the available features in the Palmer Penguins dataset. There are several variables available including Culmen size, flipper length, body mass, and the island the penguin was found on, among others. We will test different combinations of quantitative and qualitative features to determine which features are most important for classification. We will also compare the performance of different machine learning models to determine which model is best suited for this task. We will test Logistic Regression, Decision Trees, Random Forest, and Support Vector Machines, using cross-validation to ensure that our results are robust. Once the best features and model is determined, we will test the model on the test set to evaluate its performance and discuss the results using plots and performance metrics."
  },
  {
    "objectID": "posts/design-and-impact of-automated-decision-systems/index.html#summary-statistics",
    "href": "posts/design-and-impact of-automated-decision-systems/index.html#summary-statistics",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\ndf_stats = df_train.copy()\ndf_stats[\"cb_person_default_on_file\"] = (df_stats[\"cb_person_default_on_file\"] == \"Y\").astype(int)\ndf_stats[\"loan_status\"] = df_stats[\"loan_status\"].map({1: \"default\", 0: \"repaid\"})\nsummary_table = df_stats.groupby(\"loan_status\").mean(numeric_only=True)\nhome_ownership_percentages = df_stats.groupby(\"loan_status\")[\"person_home_ownership\"].value_counts(normalize=True).unstack(fill_value=0)\nloan_intent_percentages = df_stats.groupby(\"loan_status\")[\"loan_intent\"].value_counts(normalize=True).unstack(fill_value=0)\nloan_grade_percentages = df_stats.groupby(\"loan_status\")[\"loan_grade\"].value_counts(normalize=True).unstack(fill_value=0)\nfinal_summary = summary_table.join(home_ownership_percentages).join(loan_intent_percentages).join(loan_grade_percentages)\nfinal_summary.reset_index(inplace=True)\nfinal_summary.columns = final_summary.columns.str.replace('_', ' ').str.title()\nfinal_summary = final_summary.round(3)\n\ndisplay(final_summary)\n\n\n\n\n\n\n\n\nLoan Status\nPerson Age\nPerson Income\nPerson Emp Length\nLoan Amnt\nLoan Int Rate\nLoan Percent Income\nCb Person Default On File\nCb Person Cred Hist Length\nMortgage\n...\nMedical\nPersonal\nVenture\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\n0\ndefault\n27.439\n48883.177\n4.159\n10769.053\n13.061\n0.246\n0.303\n5.652\n0.236\n...\n0.224\n0.151\n0.121\n0.154\n0.236\n0.187\n0.302\n0.088\n0.024\n0.008\n\n\n1\nrepaid\n27.817\n70727.745\n4.954\n9235.881\n10.437\n0.149\n0.140\n5.834\n0.460\n...\n0.175\n0.174\n0.193\n0.381\n0.342\n0.202\n0.058\n0.014\n0.003\n0.000\n\n\n\n\n2 rows × 26 columns"
  },
  {
    "objectID": "posts/design-and-impact of-automated-decision-systems/index.html#visualizations",
    "href": "posts/design-and-impact of-automated-decision-systems/index.html#visualizations",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Visualizations",
    "text": "Visualizations\n\nfig, ax = plt.subplots(1, 3, figsize = (17, 3.5))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", hue = \"Species\", style = \"Species\", ax = ax[0])\np1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\", style = \"Species\", ax = ax[1])\np2 = sns.scatterplot(train, x = \"Delta 15 N (o/oo)\", y = \"Delta 13 C (o/oo)\",  hue = \"Species\", style = \"Species\", ax = ax[2])"
  },
  {
    "objectID": "posts/design-and-impact of-automated-decision-systems/index.html#data-preprocessing",
    "href": "posts/design-and-impact of-automated-decision-systems/index.html#data-preprocessing",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  x = df.drop([\"Species\"], axis = 1)\n  x = pd.get_dummies(df)\n  return x, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nSpecies_Adelie\nSpecies_Chinstrap\nSpecies_Gentoo\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/design-and-impact of-automated-decision-systems/index.html#variable-and-model-selection",
    "href": "posts/design-and-impact of-automated-decision-systems/index.html#variable-and-model-selection",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Variable and Model Selection",
    "text": "Variable and Model Selection\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncombs = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    combs.append(cols)\n\n–\n\nwarnings.filterwarnings('ignore')\n\ntests = []\n\nfor cols in combs:\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    LR.score(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    DTC = DecisionTreeClassifier()\n    DTC.fit(X_train[cols], y_train)\n    DTC.score(X_train[cols], y_train)\n    cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n    RFC = RandomForestClassifier()\n    RFC.fit(X_train[cols], y_train)\n    RFC.score(X_train[cols], y_train)\n    cv_scores_RFC = cross_val_score(RFC, X_train[cols], y_train, cv = 5)\n    SVM = SVC()\n    SVM.fit(X_train[cols], y_train)\n    SVM.score(X_train[cols], y_train)\n    cv_scores_SVM = cross_val_score(SVM, X_train[cols], y_train, cv = 5)\n    tests.append((cols, cv_scores_LR.mean(), cv_scores_DTC.mean(), cv_scores_RFC.mean(), cv_scores_SVM.mean()))\n\n–\n\ndf_results = pd.DataFrame(tests, columns=[\"Columns\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"])\n\ndf_results[\"Columns\"] = df_results[\"Columns\"].apply(lambda x: \", \".join(x))\n\npd.set_option(\"display.max_colwidth\", None)  # no truncation of column text\n\ndf_results = df_results.sort_values(by=\"Logistic Regression\", ascending=False) # sort by cross-val score\ndf_results.reset_index(drop=True, inplace=True)\n\ndf_results.head()\n\n\n\n\n\n\n\n\nColumns\nLogistic Regression\nDecision Tree\nRandom Forest\nSVM\n\n\n\n\n0\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n0.996154\n0.976546\n0.980468\n0.820362\n\n\n1\nCulmen Length (mm), Culmen Depth (mm), Sex_FEMALE, Sex_MALE\n0.984465\n0.972624\n0.980468\n0.808673\n\n\n2\nCulmen Length (mm), Delta 13 C (o/oo), Island_Biscoe, Island_Dream, Island_Torgersen\n0.972624\n0.964932\n0.972700\n0.722700\n\n\n3\nCulmen Length (mm), Delta 15 N (o/oo), Island_Biscoe, Island_Dream, Island_Torgersen\n0.964857\n0.968778\n0.972700\n0.730543\n\n\n4\nCulmen Length (mm), Culmen Depth (mm), Clutch Completion_No, Clutch Completion_Yes\n0.957014\n0.945249\n0.949170\n0.820362\n\n\n\n\n\n\n\n–"
  },
  {
    "objectID": "posts/design-and-impact of-automated-decision-systems/index.html#results",
    "href": "posts/design-and-impact of-automated-decision-systems/index.html#results",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Results",
    "text": "Results\n\nX_test, y_test = prepare_data(test)\nX_test.head()\nLR = LogisticRegression()\nLR.fit(X_train[LR_cols], y_train)\nLR_train_score = LR.score(X_train[LR_cols], y_train)\nLR_test_score = LR.score(X_test[LR_cols], y_test)\n\nDTC = DecisionTreeClassifier()\nDTC.fit(X_train[DT_cols], y_train)\nDT_train_score = DTC.score(X_train[DT_cols], y_train)\nDT_test_score = DTC.score(X_test[DT_cols], y_test)\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[RF_cols], y_train)\nRF_train_score = RFC.score(X_train[RF_cols], y_train)\nRF_test_score = RFC.score(X_test[RF_cols], y_test)\n\nSVM = SVC()\nSVM.fit(X_train[SVM_cols], y_train)\nSVM_train_score = SVM.score(X_train[SVM_cols], y_train)\nSVM_test_score = SVM.score(X_test[SVM_cols], y_test)\n\nscores = [[LR_train_score, LR_test_score, LR_cols], [DT_train_score, DT_test_score, DT_cols], [RF_train_score, RF_test_score, RF_cols], [SVM_train_score, SVM_test_score, SVM_cols]]\n\ndf_scores = pd.DataFrame(scores, columns=[\"Training Accuracy\", \"Testing Accuracy\", \"Variables\"])\ndf_scores[\"Model\"] = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"]\ndf_scores[\"Training Accuracy\"] = df_scores[\"Training Accuracy\"].round(3)\ndf_scores[\"Testing Accuracy\"] = df_scores[\"Testing Accuracy\"].round(3)\ndf_scores[\"Variables\"] = df_scores[\"Variables\"].apply(lambda x: \", \".join(x))\ndf_scores = df_scores[[\"Model\", \"Training Accuracy\", \"Testing Accuracy\", \"Variables\"]]\ndisplay(df_scores)\n\n\n\n\n\n\n\n\nModel\nTraining Accuracy\nTesting Accuracy\nVariables\n\n\n\n\n0\nLogistic Regression\n0.996\n1.000\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n1\nDecision Tree\n1.000\n0.985\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n2\nRandom Forest\n1.000\n0.971\nCulmen Length (mm), Flipper Length (mm), Sex_FEMALE, Sex_MALE\n\n\n3\nSVM\n0.891\n0.941\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen"
  },
  {
    "objectID": "posts/design-and-impact of-automated-decision-systems/index.html#visualizations-1",
    "href": "posts/design-and-impact of-automated-decision-systems/index.html#visualizations-1",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Visualizations",
    "text": "Visualizations\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i].replace(\"_\", \" \"))\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[LR_cols], y_train)\nplot_regions(LR, X_test[LR_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–\n\ny_test_pred = LR.predict(X_test[LR_cols])\nC = confusion_matrix(y_test, y_test_pred)\n\nclass_labels = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n\nplt.figure(figsize=(6,4))\nsns.heatmap(C, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()"
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html#summary-statistics",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html#summary-statistics",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nThe first task is to explore the data, and understand the distribution of the features as they relate to the target variable. We will start by grouping the data by the target variable and calculating the mean of each feature. This will give us an idea of how the features differ between the two groups.\n\ndf_stats = df_train.copy()\ndf_stats[\"cb_person_default_on_file\"] = (df_stats[\"cb_person_default_on_file\"] == \"Y\").astype(int)\ndf_stats[\"loan_status\"] = df_stats[\"loan_status\"].map({1: \"Default\", 0: \"Repaid\"})\nsummary_table = df_stats.groupby(\"loan_status\").mean(numeric_only=True)\nhome_ownership_percentages = df_stats.groupby(\"loan_status\")[\"person_home_ownership\"].value_counts(normalize=True).unstack(fill_value=0)\nloan_intent_percentages = df_stats.groupby(\"loan_status\")[\"loan_intent\"].value_counts(normalize=True).unstack(fill_value=0)\nfinal_summary = summary_table.join(home_ownership_percentages).join(loan_intent_percentages)\nfinal_summary.reset_index(inplace=True)\nfinal_summary.columns = final_summary.columns.str.replace('_', ' ').str.title()\nfinal_summary = final_summary.round(3)\n\ndisplay(final_summary)\n\n\n\n\n\n\n\n\nLoan Status\nPerson Age\nPerson Income\nPerson Emp Length\nLoan Amnt\nLoan Int Rate\nLoan Percent Income\nCb Person Default On File\nCb Person Cred Hist Length\nMortgage\nOther\nOwn\nRent\nDebtconsolidation\nEducation\nHomeimprovement\nMedical\nPersonal\nVenture\n\n\n\n\n0\nDefault\n27.439\n48883.177\n4.159\n10769.053\n0.131\n0.246\n0.303\n5.652\n0.236\n0.005\n0.030\n0.729\n0.212\n0.157\n0.135\n0.224\n0.151\n0.121\n\n\n1\nRepaid\n27.817\n70727.745\n4.954\n9235.881\n0.104\n0.149\n0.140\n5.834\n0.460\n0.003\n0.094\n0.443\n0.146\n0.208\n0.105\n0.175\n0.174\n0.193\n\n\n\n\n\n\n\nLooking solely at the means, we see some interesting differences between defaulted and repaid loans. For example, as one would expect, defaulted loans have a higher average loan percent of income and interest rate. This makes sense because if a loan is a larger percent of ones income, it is harder to pay off. Similarly, a higher interest rate would make it more difficult to pay off the loan. It is also interesting to see that 73% of defaulted loans are taken out by individuals who rent their homes, compared to only 44% for repaid loans. This could be due to the fact that renters have less disposable income than homeowners. Finally, the loan amount averages is curious because the average loan amount doesn’t differ that much between groups. One would expect that defaulted loans would have a higher average loan amount, but this is only slightly the case."
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html#visualizations",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html#visualizations",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Visualizations",
    "text": "Visualizations\nTo get a better idea of the characteristics of the data, we will continue by plotting some of the more interesting variables. We will scatter two continuous variables against each other, and color the points by the outcome of the loan. This will give us more of an idea of the relationships than the mean can give us. We will also calculate a line of best fit for each pairing to get a sense for whether the relationship is particularly predictive.\n\ndf_vis = df_train.copy()\ndf_vis[\"loan_status\"] = df_vis[\"loan_status\"].map({1: \"Default\", 0: \"Repaid\"})\ndf_vis.columns = df_vis.columns.str.replace('_', ' ').str.title()\n\nfig, ax = plt.subplots(1, 3, figsize=(17, 3.5))\n\n# define colors for each loan_status\npalette = [\"blue\", \"#C76E00\"]\n\np1 = sns.scatterplot(df_vis, x=\"Person Emp Length\", y=\"Person Income\", hue=\"Loan Status\", ax=ax[0])\nax[0].set_xlim(0, 42)\nax[0].set_ylim(0, 1000000)  # To remove outliers\nfor i, status in enumerate(df_vis[\"Loan Status\"].unique()):\n    subset = df_vis[df_vis[\"Loan Status\"] == status]\n    sns.regplot(data=subset, x=\"Person Emp Length\", y=\"Person Income\", scatter=False, ci=None, ax=ax[0], color=palette[i])\n\np2 = sns.scatterplot(df_vis, x=\"Loan Amnt\", y=\"Loan Percent Income\", hue=\"Loan Status\", ax=ax[1])\nfor i, status in enumerate(df_vis[\"Loan Status\"].unique()):\n    subset = df_vis[df_vis[\"Loan Status\"] == status]\n    sns.regplot(data=subset, x=\"Loan Amnt\", y=\"Loan Percent Income\", scatter=False, ci=None, ax=ax[1], color=palette[i])\n\np3 = sns.scatterplot(df_vis, x=\"Cb Person Cred Hist Length\", y=\"Loan Int Rate\", hue=\"Loan Status\", ax=ax[2])\nfor i, status in enumerate(df_vis[\"Loan Status\"].unique()):\n    subset = df_vis[df_vis[\"Loan Status\"] == status]\n    sns.regplot(data=subset, x=\"Cb Person Cred Hist Length\", y=\"Loan Int Rate\", scatter=False, ci=None, ax=ax[2], color=palette[i])\n\nplt.show()\n\n\n\n\n\n\n\n\nThe first plot of income vs employment length is interesting because it is contrary to what we observed in the summary statistics. Both income and employment length appear to have very little between them when it comes to loan status. The lines of best fit for each group almost overlap indicating that the difference between groups is marginal. This is surprising because the summary statistics showed that the average income of borrowers who repay their loans is over $20,000 higher than those who default. This is a case where means can be deceiving because a lot of that difference can be accounted for by a few outliers with very high incomes. The lines of best fit recognize this and show that the relationship is not as strong as the mean would suggest.\nThe second plot of loan amount vs loan percent of income is more in line with what we would expect. The line of best fit for defaulted loans is steeper and higher than that of repaid loans, indicating that the loan amount is more predictive of loan status than income. This makes sense because the relative size of a loan to one’s income is a better indicator of whether they can pay it back than income alone. Finally, the third plot shows the interest rate against the credit history length. The lines of best fit are very separated indicating that interest rate has a significant impact on ability to repay as we anticipated."
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html#data-preprocessing",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html#data-preprocessing",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThe next step is to train a model on the data. The first thing we need to do, in order to accomplish this, is to preprocess the data. This involves removing the target variable from the feature set and using one-hot encoding to binarize the categorical variables.\n\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  x = df.drop([\"loan_status\", \"loan_grade\"], axis = 1)\n  x = pd.get_dummies(x)\n  return x, y\n\nX_train, y_train = prepare_data(df_train)\nX_train = X_train.reset_index(drop=True)\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n0\n27\n98000\n3.0\n11750\n0.1347\n0.12\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\n22\n36996\n5.0\n10000\n0.0751\n0.27\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n24\n26000\n2.0\n1325\n0.1287\n0.05\n4\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n3\n29\n53004\n2.0\n15000\n0.0963\n0.28\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n21\n21700\n2.0\n5500\n0.1491\n0.25\n2\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse"
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html#variable-and-model-selection",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html#variable-and-model-selection",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Variable and Model Selection",
    "text": "Variable and Model Selection\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\ncombs = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    combs.append(cols)\n\n–\n\nwarnings.filterwarnings('ignore')\n\ntests = []\n\nfor cols in combs:\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    LR.score(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    DTC = DecisionTreeClassifier()\n    DTC.fit(X_train[cols], y_train)\n    DTC.score(X_train[cols], y_train)\n    cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n    RFC = RandomForestClassifier()\n    RFC.fit(X_train[cols], y_train)\n    RFC.score(X_train[cols], y_train)\n    cv_scores_RFC = cross_val_score(RFC, X_train[cols], y_train, cv = 5)\n    SVM = SVC()\n    SVM.fit(X_train[cols], y_train)\n    SVM.score(X_train[cols], y_train)\n    cv_scores_SVM = cross_val_score(SVM, X_train[cols], y_train, cv = 5)\n    tests.append((cols, cv_scores_LR.mean(), cv_scores_DTC.mean(), cv_scores_RFC.mean(), cv_scores_SVM.mean()))\n\n–\n\ndf_results = pd.DataFrame(tests, columns=[\"Columns\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"])\n\ndf_results[\"Columns\"] = df_results[\"Columns\"].apply(lambda x: \", \".join(x))\n\npd.set_option(\"display.max_colwidth\", None)  # no truncation of column text\n\ndf_results = df_results.sort_values(by=\"Logistic Regression\", ascending=False) # sort by cross-val score\ndf_results.reset_index(drop=True, inplace=True)\n\ndf_results.head()\n\n\n\n\n\n\n\n\nColumns\nLogistic Regression\nDecision Tree\nRandom Forest\nSVM\n\n\n\n\n0\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n0.996154\n0.976546\n0.980468\n0.820362\n\n\n1\nCulmen Length (mm), Culmen Depth (mm), Sex_FEMALE, Sex_MALE\n0.984465\n0.972624\n0.980468\n0.808673\n\n\n2\nCulmen Length (mm), Delta 13 C (o/oo), Island_Biscoe, Island_Dream, Island_Torgersen\n0.972624\n0.964932\n0.972700\n0.722700\n\n\n3\nCulmen Length (mm), Delta 15 N (o/oo), Island_Biscoe, Island_Dream, Island_Torgersen\n0.964857\n0.968778\n0.972700\n0.730543\n\n\n4\nCulmen Length (mm), Culmen Depth (mm), Clutch Completion_No, Clutch Completion_Yes\n0.957014\n0.945249\n0.949170\n0.820362\n\n\n\n\n\n\n\n–"
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html#results",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html#results",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Results",
    "text": "Results\n\nX_test, y_test = prepare_data(test)\nX_test.head()\nLR = LogisticRegression()\nLR.fit(X_train[LR_cols], y_train)\nLR_train_score = LR.score(X_train[LR_cols], y_train)\nLR_test_score = LR.score(X_test[LR_cols], y_test)\n\nDTC = DecisionTreeClassifier()\nDTC.fit(X_train[DT_cols], y_train)\nDT_train_score = DTC.score(X_train[DT_cols], y_train)\nDT_test_score = DTC.score(X_test[DT_cols], y_test)\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[RF_cols], y_train)\nRF_train_score = RFC.score(X_train[RF_cols], y_train)\nRF_test_score = RFC.score(X_test[RF_cols], y_test)\n\nSVM = SVC()\nSVM.fit(X_train[SVM_cols], y_train)\nSVM_train_score = SVM.score(X_train[SVM_cols], y_train)\nSVM_test_score = SVM.score(X_test[SVM_cols], y_test)\n\nscores = [[LR_train_score, LR_test_score, LR_cols], [DT_train_score, DT_test_score, DT_cols], [RF_train_score, RF_test_score, RF_cols], [SVM_train_score, SVM_test_score, SVM_cols]]\n\ndf_scores = pd.DataFrame(scores, columns=[\"Training Accuracy\", \"Testing Accuracy\", \"Variables\"])\ndf_scores[\"Model\"] = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"SVM\"]\ndf_scores[\"Training Accuracy\"] = df_scores[\"Training Accuracy\"].round(3)\ndf_scores[\"Testing Accuracy\"] = df_scores[\"Testing Accuracy\"].round(3)\ndf_scores[\"Variables\"] = df_scores[\"Variables\"].apply(lambda x: \", \".join(x))\ndf_scores = df_scores[[\"Model\", \"Training Accuracy\", \"Testing Accuracy\", \"Variables\"]]\ndisplay(df_scores)\n\n\n\n\n\n\n\n\nModel\nTraining Accuracy\nTesting Accuracy\nVariables\n\n\n\n\n0\nLogistic Regression\n0.996\n1.000\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n1\nDecision Tree\n1.000\n0.985\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen\n\n\n2\nRandom Forest\n1.000\n0.971\nCulmen Length (mm), Flipper Length (mm), Sex_FEMALE, Sex_MALE\n\n\n3\nSVM\n0.891\n0.941\nCulmen Length (mm), Culmen Depth (mm), Island_Biscoe, Island_Dream, Island_Torgersen"
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html#visualizations-1",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html#visualizations-1",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Visualizations",
    "text": "Visualizations\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i].replace(\"_\", \" \"))\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[LR_cols], y_train)\nplot_regions(LR, X_test[LR_cols], y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–\n\ny_test_pred = LR.predict(X_test[LR_cols])\nC = confusion_matrix(y_test, y_test_pred)\n\nclass_labels = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n\nplt.figure(figsize=(6,4))\nsns.heatmap(C, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()"
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "In this blog post, we will be exploring bank loan data, specifically inquiring about the ability to predict loan success based on borrower characteristics. The data includes information about the borrower, loan amount, loan status, and other relevant information. Loan status is the target variable, where 1 indicates a default and 0 means the loan was repaid. We will test different combinations of features with a Logistic Regression model to discover the most predictive indicators of loan success. We will then discuss the results including an investigation of how different groups would be affected if the model were to be implemented."
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html#feature-selection",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html#feature-selection",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Feature Selection",
    "text": "Feature Selection\nWith the data processed, we now need to determine which features to include in the model. To do this we will perform an exhaustive search of every combination of 3 features and use cross-validation scores to compare them. The following code will perform these tests by training a logistic regression model on each combination of features and calculating the cross-validation score. The best combination of features will be the one with the highest cross-validation score.\n\nwarnings.filterwarnings('ignore')\nattributes = X_train.columns\ncombs = list(combinations(attributes, 3))\n\ntests = []\n\nfor cols in combs:\n    LR = LogisticRegression()\n    LR.fit(X_train[list(cols)], y_train)\n    LR.score(X_train[list(cols)], y_train)\n    cv_scores = cross_val_score(LR, X_train[list(cols)], y_train, cv = 5)\n    tests.append((list(cols), cv_scores.mean()))\n\nTo find the best combination we will sort by cross-validation score and display the top 5 combinations.\n\ndf_results = pd.DataFrame(tests, columns=[\"Columns\", \"Cross-Validation Score\"])\n\ndf_results[\"Columns\"] = df_results[\"Columns\"].apply(lambda x: \", \".join(x))\n\npd.set_option(\"display.max_colwidth\", None)  # no truncation of column text\n\ndf_results = df_results.sort_values(by=\"Cross-Validation Score\", ascending=False) # sort by cross-val score\ndf_results.reset_index(drop=True, inplace=True)\n\ndf_results.head()\n\n\n\n\n\n\n\n\nColumns\nCross-Validation Score\n\n\n\n\n0\nloan_percent_income, person_home_ownership_RENT, loan_intent_HOMEIMPROVEMENT\n0.849521\n\n\n1\nloan_percent_income, person_home_ownership_RENT, loan_intent_DEBTCONSOLIDATION\n0.849260\n\n\n2\nloan_percent_income, person_home_ownership_OTHER, person_home_ownership_RENT\n0.849216\n\n\n3\nloan_percent_income, person_home_ownership_MORTGAGE, person_home_ownership_OWN\n0.848736\n\n\n4\nloan_percent_income, person_home_ownership_RENT, loan_intent_MEDICAL\n0.848430\n\n\n\n\n\n\n\nAs the above table shows, the combination of loan percent of income, person home ownership: rent, and loan intent: home improvement performs the best with a cross-validation score of 0.8495. This makes sense with our initial exploration as we recognized the big difference between groups in loan percent of income and home ownership status.\nWith the best combination of features determined, we can now train the model on the entire dataset and evaluate its performance. By training a logistic regression model on the training columns we are able to obtain a weight vector which can be used to score new data.\n\ncols = df_results[\"Columns\"].iloc[0].split(\", \")\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR_train_score = LR.score(X_train[cols], y_train)\nprint(f\"Training Score: {LR_train_score}\")\nw = LR.coef_[0]\nprint(f\"Weight Vector (w): {w}\")\n\nTraining Score: 0.8493473610686689\nWeight Vector (w): [8.14642112 1.19649321 0.49192603]\n\n\nAbove we see the training score which matches the cross-validation score, as expected, as well as the weight vector which we will use in the next section to find a threshold."
  },
  {
    "objectID": "posts/design-and-impact-of-automated-decision-systems/index.html#evaluate-from-the-borrowers-perspective",
    "href": "posts/design-and-impact-of-automated-decision-systems/index.html#evaluate-from-the-borrowers-perspective",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Evaluate from the Borrower’s Perspective",
    "text": "Evaluate from the Borrower’s Perspective\nWe next want to consider how the implementation of this model would affect borrowers. We will first look at different age groups by binning the ages and calculating the acceptance rate for each group. We will then calculate the acceptance rate for different income levels and loan purposes.\n\nbins = [X_test[\"person_age\"].min(), 25, 35, 45, 55, 65, X_test[\"person_age\"].max()]\nlabels = [\"&lt;25\", \"25-35\", \"35-45\", \"45-55\", \"55-65\", \"65+\"]\n\nX_test[\"age_group\"] = pd.cut(X_test[\"person_age\"], bins=bins, labels=labels, include_lowest=True)\n\nacceptance_rates_age = X_test.groupby(\"age_group\").apply(lambda group: np.mean(preds[group.index] == 0))\n\nplt.figure(figsize=(8, 5))\nsns.barplot(x=acceptance_rates_age.index, y=acceptance_rates_age.values, palette=\"Blues_r\")\n\nplt.xlabel(\"Age Group\")\nplt.ylabel(\"Acceptance Rate\")\nplt.title(\"Acceptance Rates by Age\")\nplt.ylim(0.75, 1)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\nAs the plot above shows, the acceptance rate is a bell curve with people between 35 and 55 having the highest acceptance rate. This makes logical sense because younger people are less settled and thus more risky to lend to while death becomes a worry when lending to older people. People in the middle are more likely to have stable jobs and thus be able to pay back the loan.\n\nX_test[\"predict_default\"] = preds\nX_test[\"loan_status\"] = y_test\n\nmedical = X_test.groupby(\"loan_intent_MEDICAL\")[[\"predict_default\", \"loan_status\"]].mean().reset_index()\nventure = X_test.groupby(\"loan_intent_VENTURE\")[[\"predict_default\", \"loan_status\"]].mean().reset_index()\neducation = X_test.groupby(\"loan_intent_EDUCATION\")[[\"predict_default\", \"loan_status\"]].mean().reset_index()\n\nmedical = medical.rename(columns={\"loan_intent_MEDICAL\": \"Medical\", \"predict_default\": \"Predicted Default Rate\", \"loan_status\": \"Actual Default Rate\"})\nventure = venture.rename(columns={\"loan_intent_VENTURE\": \"Business Venture\", \"predict_default\": \"Predicted Default Rate\", \"loan_status\": \"Actual Default Rate\"})\neducation = education.rename(columns={\"loan_intent_EDUCATION\": \"Education\", \"predict_default\": \"Predicted Default Rate\", \"loan_status\": \"Actual Default Rate\"})\n\ndisplay(medical, venture, education)\n\n\n\n\n\n\n\n\nMedical\nPredicted Default Rate\nActual Default Rate\n\n\n\n\n0\nFalse\n0.086518\n0.208673\n\n\n1\nTrue\n0.103448\n0.284250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Venture\nPredicted Default Rate\nActual Default Rate\n\n\n\n\n0\nFalse\n0.091252\n0.238305\n\n\n1\nTrue\n0.081950\n0.146266\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation\nPredicted Default Rate\nActual Default Rate\n\n\n\n\n0\nFalse\n0.093304\n0.237102\n\n\n1\nTrue\n0.075680\n0.167517\n\n\n\n\n\n\n\nLooking at the default predictions by loan intent and comparing them with the actual outcome is also interesting. The predicted default rate for medical intended loans is higher than other loans, and the actual default rate reflects this trend as well. This makes sense because medical loans are often taken out of necessity rather than choice so the individuals involved don’t have the luxury to consider whether or not they will be able to pay them back. Loans for business ventures and education, on the other hand, have a lower predicted default rate and this is reflected in the actual default rate as well. This is likely because these loans are taken out with the intention of making money in the future, so if that comes to pass these individuals will be able to repay their loans more easily.\n\nquartiles, bin_edges = pd.qcut(X_test[\"person_income\"], q=6, retbins=True, labels=False)\n\nlabels = [f\"${int(bin_edges[i])}-{int(bin_edges[i+1])}\" for i in range(len(bin_edges)-1)]\nlabels[0] = f\"&lt;${int(bin_edges[1])}\"\nlabels[-1] = f\"&gt;${int(bin_edges[-2])}\"\n\nX_test[\"income_group\"] = pd.qcut(X_test[\"person_income\"], q=6, labels=labels)\n\nacceptance_rates_income = X_test.groupby(\"income_group\").apply(lambda group: np.mean(preds[group.index] == 0))\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=acceptance_rates_income.index, y=acceptance_rates_income.values, palette=\"Blues_r\")\n\nplt.xlabel(\"Income Group\")\nplt.ylabel(\"Acceptance Rate\")\nplt.title(\"Acceptance Rates by Income Level\")\nplt.ylim(0.75, 1)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n\n\n\n\n\n\n\nFinally, we will look at acceptance rates by income level. As the chart above shows, this is a pretty linear relationship with higher income individuals being more likely to be accepted for a loan. This makes sense because higher income individuals are more likely to be able to pay back the loan."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Abstract\nIn this blog post, we explore the ability of machine learning models to predict employment among individuals in Colorado, using data from the American Community Survey’s Public Use Microdata Sample (PUMS). We will use a polynomial logistic regression model to predict employment status based on a variety of demographic and economic factors. We will exclude race from the training so that we can then determine if racial biases are still exhibited despite not explicitly including race. We find that racial biases are still present in the model, and conclude that predictive models should be carefully evaluated for biases before being used in decision-making processes.\n\n\nData Organization\nSince I am from Denver, I will be using Colorado as the source of my data. the following code pulls the data from Colorado for the PUMS survey.\n\nSTATE = \"CO\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000225\n8\n1\n300\n4\n8\n1013097\n26\n16\n...\n23\n4\n24\n33\n53\n46\n5\n4\n47\n42\n\n\n1\nP\n2018GQ0000483\n8\n1\n4103\n4\n8\n1013097\n41\n19\n...\n7\n40\n39\n43\n41\n6\n41\n6\n41\n44\n\n\n2\nP\n2018GQ0000529\n8\n1\n4101\n4\n8\n1013097\n16\n24\n...\n31\n16\n1\n32\n17\n15\n16\n17\n2\n18\n\n\n3\nP\n2018GQ0000670\n8\n1\n818\n4\n8\n1013097\n3\n22\n...\n6\n3\n2\n7\n3\n4\n5\n3\n3\n3\n\n\n4\nP\n2018GQ0000721\n8\n1\n819\n4\n8\n1013097\n17\n65\n...\n22\n0\n17\n37\n0\n0\n30\n16\n17\n1\n\n\n\n\n5 rows × 286 columns\n\n\n\nThe next step is to remove the target column (employment status) and race column from the dataset. We are trying to predict employment status, so that can’t be one of the features we train on. Additionally, we are trying to test if a model predicting employment status is racially biased without training on race so we need to remove race too. The following code splits the features and creates a BasicProblem object based on those features.\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nNext we need to split our data into a training and testing set. The following code does that using train_test_split from sklearn to create a test set that is 20% of the data.\n\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nBasic Descriptives\nNext we want to examine the training data to better understand the proportions as they relate to employment status and race along with sex. Looking at these intersectional trends will give us a sense of the baseline we are working with. We will first find the number of individuals in the dataset, and then the number of individuals that identify as either White or Black, then the employment proportions of both groups. Finally we will look at intersectional trends between race and sex.\n\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"RACE\"] = group_train\ndf[\"label\"] = y_train\n\nnum_individuals = len(df)\nprint(f\"Number of individuals in the data: {num_individuals}\\n\")\n\nproportion_target_1 = df['label'].mean()\nprint(f\"Proportion of individuals with target label equal to 1: {proportion_target_1:.2f}\\n\")\n\ndf = df[df[\"RACE\"].isin([1, 2])]\ndf[\"RACE\"] = df[\"RACE\"].map({1: \"White\", 2: \"Black\"})\ndf[\"SEX\"] = df[\"SEX\"].map({1: \"Male\", 2: \"Female\"})\n\ngroup_counts = df['RACE'].value_counts()\nprint(\"Number of individuals in each group:\")\ndisplay(group_counts.to_frame(name='count'))\n\ngroup_proportion = df.groupby('RACE')['label'].mean()\nprint(\"Proportion of positive target labels by race:\")\ndisplay(group_proportion.to_frame(name='Employment Proportion'))\n\nintersectional_proportion = df.groupby(['RACE', 'SEX'])['label'].mean().unstack()\nprint(\"Proportion of positive target labels by race and sex:\")\ndisplay(intersectional_proportion)\n\nintersectional_proportion = intersectional_proportion.reset_index().melt(id_vars='RACE', value_name='proportion')\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='RACE', y='proportion', hue='SEX', data=intersectional_proportion)\nplt.xlabel('Race')\nplt.ylabel('Proportion of Positive Target Labels')\nplt.title('Proportion of Positive Target Labels by Race and Sex')\nplt.legend(title='Sex')\nplt.show()\n\nNumber of individuals in the data: 44742\n\nProportion of individuals with target label equal to 1: 0.50\n\nNumber of individuals in each group:\n\n\n\n\n\n\n\n\n\ncount\n\n\nRACE\n\n\n\n\n\nWhite\n38951\n\n\nBlack\n1264\n\n\n\n\n\n\n\nProportion of positive target labels by race:\n\n\n\n\n\n\n\n\n\nEmployment Proportion\n\n\nRACE\n\n\n\n\n\nBlack\n0.434335\n\n\nWhite\n0.504737\n\n\n\n\n\n\n\nProportion of positive target labels by race and sex:\n\n\n\n\n\n\n\n\nSEX\nFemale\nMale\n\n\nRACE\n\n\n\n\n\n\nBlack\n0.405676\n0.460150\n\n\nWhite\n0.473525\n0.535995\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, there are 44,742 people in the dataset, 38,951 of which are White and 1,264 of which are Black. Overall, exactly half of the people are employed, and the employment proportions are 50.47% for White individuals and 43.43% for Black individuals. For both groups, the proportion of women who are employed is lower than the proportion men with 53.60% of White men employed compared to 47.35% of White women and 46.02% of Black men compared to 40.57% of Black women. The gaps between groups in interesting and illuminates the potential racial and sex-based biases in employment in Colorado.\n\n\nModel Training\nNext we will train a model on the training data and evaluate it on the test data. We will use a logistic regression model, but we are interested in the potential of polynomial features to improve to predictions. To test the effect of polynomial features we will use cross validation testing with a range of polynomial degrees. The following code does this and outputs the results.\n\nwarnings.filterwarnings('ignore')\ndegrees = range(1, 5)\ntests = []\n\nfor degree in degrees:\n    poly = PolynomialFeatures(degree)\n    X_train_poly = poly.fit_transform(X_train)\n    LR = LogisticRegression()\n    LR.fit(X_train_poly, y_train)\n    LR.score(X_train_poly, y_train)\n    cv_scores = cross_val_score(LR, X_train_poly, y_train, cv = 5)\n    tests.append((degree, cv_scores.mean()))\n\ndf_results = pd.DataFrame(tests, columns=[\"Degree\", \"Cross-Validation Score\"])\ndf_results.reset_index(drop=True, inplace=True)\ndisplay(df_results)\n\n\n\n\n\n\n\n\nDegree\nCross-Validation Score\n\n\n\n\n0\n1\n0.787962\n\n\n1\n2\n0.815163\n\n\n2\n3\n0.814380\n\n\n3\n4\n0.758460\n\n\n\n\n\n\n\nAs we can see, the Logistic Regression with degree of 2 performs the best, closely followed by degree 3. Given this we will use degree 2 for the rest of the analysis. The following code fits a degree 2 logistic regression model on the training data and evaluates it on the test data, outputting the accuracy.\n\nmodel = make_pipeline(PolynomialFeatures(2), LogisticRegression())\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\nprint(f\"Accuracy: {(y_hat == y_test).mean()}\")\n\nAccuracy: 0.8146790631146076\n\n\nAs we can see, the model has an accuracy of 0.815 which is very similar to the cross validation results. This is a good sign that the model is generalizing well to the test data.\n\n\nBias Audit\nThe next step is to audit the results for bias. The first step is to understand the PPV and FPR rates for the overall model so that we can compare those to the rates for each group. The following code does this.\n\ntn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n\nppv = tp / (tp + fp)\nprint(f\"Positive Predictive Value (PPV): {ppv:.2f}\")\n\nfpr = fp / (fp + tn)\nfnr = fn / (fn + tp)\nprint(f\"False Positive Rate (FPR): {fpr:.2f}\")\nprint(f\"False Negative Rate (FNR): {fnr:.2f}\")\n\nPositive Predictive Value (PPV): 0.78\nFalse Positive Rate (FPR): 0.25\nFalse Negative Rate (FNR): 0.12\n\n\nAs we can see, the overall model has a PPV of 0.78, a FPR of 0.25, and a FNR of 0.12, along with the aforementioned accuracy of 0.82. Next we will calculate these metrics for each group. The following code does this.\n\nprint(f\"Accuracy for White Individuals {(y_hat == y_test)[group_test == 1].mean()}\")\nprint(f\"Accuracy for Black Individuals {(y_hat == y_test)[group_test == 2].mean()}\")\n\nAccuracy for White Individuals 0.8162256769278287\nAccuracy for Black Individuals 0.756578947368421\n\n\nThese results show a small difference between groups. While the accuracy for White individuals is almost exactly in line with the overall model, the accuracy for Black individuals is 6 points lower. Statistical tests would be needed to determine if this difference is statistically significant, but these results suggest that the model may be biased against Black individuals.\nNext we will calculate the PPV, FPR, and FNR for each group. The following code does this.\n\nX_test_df = pd.DataFrame(X_test, columns=features_to_use)\nX_test_df[\"Race\"] = group_test\nX_test_df[\"Employment Prediction\"] = y_hat\nX_test_df[\"Employment Actual\"] = y_test\nX_test_df = X_test_df[X_test_df[\"Race\"].isin([1, 2])]\nX_test_df[\"Race\"] = X_test_df[\"Race\"].map({1: \"White\", 2: \"Black\"})\n\nX_test_df_white = X_test_df[X_test_df[\"Race\"] == \"White\"]\nX_test_df_black = X_test_df[X_test_df[\"Race\"] == \"Black\"]\n\ny_test_white = X_test_df_white[\"Employment Actual\"]\ny_hat_white = X_test_df_white[\"Employment Prediction\"]\ny_test_black = X_test_df_black[\"Employment Actual\"]\ny_hat_black = X_test_df_black[\"Employment Prediction\"]\n\ntn_white, fp_white, fn_white, tp_white = confusion_matrix(y_test_white, y_hat_white).ravel()\ntn_black, fp_black, fn_black, tp_black = confusion_matrix(y_test_black, y_hat_black).ravel()\n\nppv_white = tp_white / (tp_white + fp_white)\nppv_black = tp_black / (tp_black + fp_black)\nprint(f\"Positive Predictive Value (PPV) for White Individuals: {ppv_white:.2f}\")\nprint(f\"Positive Predictive Value (PPV) for Black Individuals: {ppv_black:.2f}\\n\")\n\nfpr_white = fp_white / (fp_white + tn_white)\nfnr_white = fn_white / (fn_white + tp_white)\nfpr_black = fp_black / (fp_black + tn_black)\nfnr_black = fn_black / (fn_black + tp_black)\nprint(f\"False Positive Rate (FPR) for White Individuals: {fpr_white:.2f}\")\nprint(f\"False Negative Rate (FNR) for White Individuals: {fnr_white:.2f}\")\nprint(f\"False Positive Rate (FPR) for Black Individuals: {fpr_black:.2f}\")\nprint(f\"False Negative Rate (FNR) for Black Individuals: {fnr_black:.2f}\")\n\nPositive Predictive Value (PPV) for White Individuals: 0.78\nPositive Predictive Value (PPV) for Black Individuals: 0.66\n\nFalse Positive Rate (FPR) for White Individuals: 0.25\nFalse Negative Rate (FNR) for White Individuals: 0.12\nFalse Positive Rate (FPR) for Black Individuals: 0.28\nFalse Negative Rate (FNR) for Black Individuals: 0.20\n\n\nAs we can see, the PPV for White individuals is 0.78 and the PPV for Black individuals is 0.66. The FPR for White individuals is 0.25 and the FPR for Black individuals is 0.28. The FNR for White individuals is 0.12 and the FNR for Black individuals is 0.20. These results suggest that the model is biased against Black individuals, with a lower PPV and higher FNR compared to White individuals.\nWe next want to determine if the model is approximately calibrated. To do this we will check if the predicted probabilities match and don’t depend on group membership. The following code does this.\n\ncalibration_white = X_test_df_white.groupby(\"Employment Prediction\")[\"Employment Actual\"].mean()\ncalibration_black = X_test_df_black.groupby(\"Employment Prediction\")[\"Employment Actual\"].mean()\ncalibration_white.columns = [\"Employment Prediction\", \"White\"]\ncalibration_black.columns = [\"Employment Prediction\", \"Black\"]\ncalibration_combined = pd.merge(calibration_white, calibration_black, on=\"Employment Prediction\", how=\"outer\")\ncalibration_combined.columns = [\"Employment Actual - White\", \"Employment Actual - Black\"]\ndisplay(calibration_combined)\n\nprint(f\"Predicted Employement Proportion (White): {y_hat_white.mean():.2f}\")\nprint(f\"Predicted Employement Proportion (Black): {y_hat_black.mean():.2f}\")\n\n\n\n\n\n\n\n\nEmployment Actual - White\nEmployment Actual - Black\n\n\nEmployment Prediction\n\n\n\n\n\n\nFalse\n0.137581\n0.154839\n\n\nTrue\n0.781867\n0.664430\n\n\n\n\n\n\n\nPredicted Employement Proportion (White): 0.57\nPredicted Employement Proportion (Black): 0.49\n\n\nHere, we see that the model is not perfectly calibrated, and that, given a True prediction, it is more accurate for White individuals than Black individuals. This means that the model is not approximately calibrated and is likely biased against Black individuals.\nThe next question relates to error rate balance. Once again, we would need statistical significance tests to determine if the differences are significant, but the previous FPR and FNR results suggest that the model is somewhat biased against Black individuals with an 8 percentage point higher false negative rate. That said, Black individuals also have a 3 percentage point higher false positive rate, so the overall effect is unclear.\nFinally, we want to look at statistical parity. The previous results demonstrate that this is not satisfied as the positive prediction rate for White individuals is 8 percentage points higher than that of Black individuals. This suggests that the model is biased against Black individuals.\nThe final step is to look at the feasible FNR and FPR rates to determine the ceiling for fairness of this model. The following code calculates and plots the results.\n\nprevalence_white = y_test_white.mean()\nprevalence_black = y_test_black.mean()\n\nprint(f\"Prevalence for White individuals: {prevalence_white:.2f}\")\nprint(f\"Prevalence for Black individuals: {prevalence_black:.2f}\")\n\ndesired_ppv_white = ppv_white\ndesired_ppv_black = ppv_black\n\nfnr_values = np.linspace(0, 1, 100)\n\nfpr_line_white = (prevalence_white / (1 - prevalence_white))*((1 - desired_ppv_white) / desired_ppv_white) * (1 - fnr_values)\nfpr_line_black = (prevalence_black / (1 - prevalence_black))*((1 - desired_ppv_black) / desired_ppv_black) * (1 - fnr_values)\n\nplt.figure(figsize=(10, 6))\nplt.plot(fnr_values, fpr_line_white, label='White', color='blue')\nplt.plot(fnr_values, fpr_line_black, label='Black', color='red')\nplt.scatter(fpr_white, fnr_white, color='blue')\nplt.scatter(fpr_black, fnr_black, color='red')\nplt.xlabel('False Negative Rate (FNR)')\nplt.ylabel('False Positive Rate (FPR)')\nplt.title('Feasible (FNR, FPR) Combinations')\nplt.legend()\nplt.grid(True)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.show()\n\nPrevalence for White individuals: 0.51\nPrevalence for Black individuals: 0.40\n\n\n\n\n\n\n\n\n\nHolding the prevalence and PPV values fixed at their observed values, in order for false positive rates to be equal between groups, the false negative rate for Black individuals would have to be around 0.70. You can see this by looking at the point for White individuals which represents the observed FNR and FPR values. The lines show feasible combinations, so to get the FPR for Black individuals to equal the observed White FPR, the FNR would have to be 0.70. This is obviously not desirable because it would mean that the model is losing a lot of accuracy. This demonstrates the tradeoff between error rate ballance and positive predictive value in models, as you can’t have both.\n\n\nDiscussion\nBanks trying to determine whether to offer loans could stand to benefit from a system that predicts employment status. Employed individuals are much more likely to pay back their loans since they have a steady income, so if a bank could predict whether an individual is employed that would help simplify their loaning decisions.\nThe bias audit illuminates that models based on the data available can easily become racially biased, favoring White people over their Black counterparts. Implementing such a model would greatly disadvantage Black individuals causing the unfair system to be perpetuated. This is a clear example of how machine learning models can perpetuate and exacerbate existing biases in society.\nThis model definitely displays problematic bias since it is biased against Black individuals. It fails error rate balance as well as sufficiency, calibration, and statistical parity as discussed above.\nThere are other potential problems associated with deploying such a model. As we saw in the brief intersectional analysis, the employment rates are also different between sexes. This means that the model could also be biased against certain sexes in addition to races and likely other groups as well. There is so much bias in society already so by using past data that reflects this, machine learning models can easily become biased in the same ways, perpetuating the problems."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html",
    "href": "posts/implementing-the-perceptron-algorithm/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Link to perception implementation: perceptron.py"
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#understanding-perceptron.grad",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#understanding-perceptron.grad",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Understanding perceptron.grad()",
    "text": "Understanding perceptron.grad()\nTo implement the perceptron algorithm we use gradient descent and the grad function of the algorithm performs this for us. The first step to do this is to convert the output variables from 0 and 1 to -1 and 1 so that the math works. Once this is done we calculate scored for the observations. We then compare these scores to the actual labels to identify the misclassified points. We then use matrix multiplication with X and y to calculate the gradient. The matrix multiplication works to effectively calculate a weighted sum of the feature vectors for the misclassified points. This is then normalized by the number of points and multiplied by the learning rate to get the gradient. The gradient is then returned and is used by the step function to update the weights."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#check-implementation",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#check-implementation",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Check Implementation",
    "text": "Check Implementation\nBefore we move to the experiments we want to ensure that our algorithm is working as intended. To do this we will perform a minimal training loop to ensure that the loss converges to 0 on linearly separable data.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nX, y = perceptron_data(n_points = 50, noise = 0.3, separable = True)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y)\n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.figure(figsize=(8, 6))\nplt.plot(loss_vec, marker='o', color='blue', label='Loss', markersize=4)\nplt.xlabel('Perceptron Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Number of Iterations')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the above plot, the perceptron algorithm is able to correctly work its way to a loss of zero indicating to us that the algorithm is working as intended."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#experiment-1",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#experiment-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 1",
    "text": "Experiment 1\nThe first experiment involves linearly separable 2-dimensional data. We will create a dataset with 50 points and plot it as shown below.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data(n_points = 50, noise = 0.3, separable = True)\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nWith this data we will run our algorithm and visualize the results. We will plot the data and the decision boundary along with the previous decision boundary as the iterations continue to improve the loss. To perform the training we are selecting a random point and seeing if it was misclassified. If it was we call the step function to update the weights. The loop stops when the loss is zero indicating that the data has been completely separated by the threshold.\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAs we can see, the algorithm works from a random guess to a threshold that perfectly separates the data. The final plot shows this separation and the loss of zero confirms that the data has been completely separated."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#experiment-2",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#experiment-2",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 2",
    "text": "Experiment 2\nThe second experiment involves 2-dimensional data that is not linearly separable. We will create a dataset with 50 points and plot it as shown below to confirm that it is not linearly separable.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data(n_points = 50, noise = 0.3, separable = False)\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nSince the data cannot be perfectly separated, the algorithm will run forever and never reach zero loss. To rectify this we will limit the algorithm to 1000 iterations and plot the final result along with some intermediate results.\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\nto_graph = [0, 199, 399, 599, 799, 999]\n\nfor j in range(1000):\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if j in to_graph:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"{j + 1} - Loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAs we see in the plot, the algorithm ran until the maximum number of iterations and the final result has a loss greater than zero. The algorithm was unable to perfectly separate the data as expected. That said, its final result is still a better decision boundary than its initial guess indicating that it was still able to learn and improve."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#experiment-3",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#experiment-3",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 3",
    "text": "Experiment 3\nThe final experiment involves linearly separable 5-dimensional data. We will create a dataset with 50 points but we cannot plot it due to the high dimensionality. We will run the algorithm and plot the loss over time to see how it converges by plotting the loss over time.\n\nX, y = perceptron_data(n_points = 50, noise = 0.3, p_dims = 5, separable = True)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n\niterations = range(1, len(loss_vec) + 1)  # Create a range for the number of iterations\n\n# Plot the loss vector\nplt.figure(figsize=(8, 6))\nplt.plot(iterations, loss_vec, marker='o', color='blue', label='Loss', markersize=4)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Number of Iterations')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs the plot above shows, the loss decreases over the iterations until it finally reaches zero and the algorithm stops. This indicates that the algorithm was able to perfectly separate the data in the 5-dimensional space so the data was linearly separable."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#experiment-1-1",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#experiment-1-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 1",
    "text": "Experiment 1\nThe first experiment involves linearly separable 2-dimensional data and a k value of 1. The point of this experiment is to ensure that the algorithm is still working as expected and the changes haven’t impacted the results.\n\nX, y = perceptron_data(n_points = 50, noise = 0.3, p_dims = 2, separable = True)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nk = 1\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    ix = torch.randperm(X.size(0))[:k]\n    X_batch = X[ix, :]\n    y_batch = y[ix]\n    local_loss = p.loss(X_batch, y_batch).item()\n\n    if local_loss &gt; 0:\n        opt.step(X_batch, y_batch)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAs we can see from the above plot, the mini-batch perceptron algorithm is able to correctly work its way to a loss of zero indicating to us that the algorithm still works as intended when it is not doing any batching."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#experiment-2-1",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#experiment-2-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 2",
    "text": "Experiment 2\nNext we want to test the batching on linearly-separable data to see if the algorithm works with batching. We will use a k value of 10 and visualize the results.\n\nX, y = perceptron_data(n_points = 50, noise = 0.3, p_dims = 2, separable = True)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nk = 10\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    ix = torch.randperm(X.size(0))[:k]\n    X_batch = X[ix, :]\n    y_batch = y[ix]\n    local_loss = p.loss(X_batch, y_batch).item()\n\n    if local_loss &gt; 0:\n        opt.step(X_batch, y_batch)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAs the plot shows, the algorithm is able to correctly separate the data with a batch size of 10. The loss converges to zero and the decision boundary is able to separate the data as expected."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#experiment-3-1",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#experiment-3-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Experiment 3",
    "text": "Experiment 3\nThe final experiment involves a k value of n where n is the number of data points. We will test this on non linearly separable data to see if it is able to converge. Our convergence test can no longer be a loss of zero since that is impossible with data that is not linearly separable. Instead we will use a maximum number of iterations to stop the algorithm and plot the results to see if we observe convergence.\n\nn = 50\nX, y = perceptron_data(n_points = n, noise = 0.3, p_dims = 2, separable = False)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\nk = n\nlr = 0.1\nloss = 1\nloss_vec = []\n\nfor _ in range(500):\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    ix = torch.randperm(X.size(0))[:k]\n    X_batch = X[ix, :]\n    y_batch = y[ix]\n    local_loss = p.loss(X_batch, y_batch).item()\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\n    if local_loss &gt; 0:\n        opt.step(X_batch, y_batch, lr)\n\niterations = range(1, len(loss_vec) + 1)  # Create a range for the number of iterations\n\n# Plot the loss vector\nplt.figure(figsize=(8, 6))\nplt.plot(iterations, loss_vec, marker='o', color='blue', label='Loss', markersize=4)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Number of Iterations')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, while it does not converge to zero lose, the algorithm is able to converge to a loss of around 0.06. This indicates that the algorithm is able to learn and improve the decision boundary even when the data is not linearly separable and this works with the mini-batching."
  },
  {
    "objectID": "posts/implementing-the-perceptron-algorithm/index.html#runtime",
    "href": "posts/implementing-the-perceptron-algorithm/index.html#runtime",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Runtime",
    "text": "Runtime\nA single iteration involves computing the score for each data point which is done with matrix multiplication meaning it takes O(n x p) time for n data points and p features. It then checks for misclassified points which involves searching the n scores which means that this check takes O(n) time. For the weight update, the calculation looks at all of the features meaning for each misclassified point it takes O(p) time. This means that the weight update takes O(n x p) time in the worst case where all of the points are misclassified. Since these steps occur consecutively, the overall runtime is O(n x p) + O(p) + O(n x p) which simplifies to O(n x p) time. For the mini-batch implementation, it also depends on the value of k since it does all of this work for k points at a time. This means that the runtime is O(k x n x p) time."
  }
]